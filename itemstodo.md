# RAG Platform Roadmap - Items To Do

**Last Updated**: 2025-12-06  
**Status**: Active Planning

---

## ‚ö° **P0 ‚Äî CRITICAL PERFORMANCE (CPU Optimization)**

### 0Ô∏è‚É£ CPU-Only LLM Performance Optimization

**Why:** LLM generation taking 4.4 minutes (264 seconds) is unacceptable for production.

**Completed:**
* ‚úÖ Context window: 2048 ‚Üí 512 (40-60% faster)
* ‚úÖ Threading: Auto ‚Üí Explicit (cores-1) (30-50% faster)
* ‚úÖ Memory: use_mmap=True, n_threads_batch (5-15% faster)
* ‚úÖ Code changes committed to `api_server.py`

**Pending:**
* ‚ö†Ô∏è Apply `api_server.py` changes to Docker container
* ‚ö†Ô∏è Update Docker run command with CPU/memory limits (`--cpus="8" --memory="8g"`)

**Expected Impact:** 4-8x faster (264s ‚Üí 30-60s)

**Priority:** ‚ö° P0 (Critical Performance)

**Status:** ‚úÖ Code Complete | ‚ö†Ô∏è Needs Deployment

**Documentation:**
* `CPU_OPTIMIZATION_GUIDE.md` - Detailed implementation guide
* `OPTIMIZATION_STATUS.md` - Current status and deployment steps
* `CPU_OPTIMIZATION_REVIEW.md` - Review of additional recommendations

---

## üî• **P0 ‚Äî CRITICAL (Do These First)**

These directly protect **correctness, safety, and hallucination control** at enterprise scale.

### 1Ô∏è‚É£ Multi-Intent Query Execution & Prompt Fusion

**Why:** Users will ask:

> "What is the schema and today's errors for dda_transactions?"

‚úÖ You already detect multi-intent

‚ùå You still need:

* Parallel vector searches per `doc_sub_type`
* Merge results into a **single structured prompt**
* Keep strict grouping:

  ```
  [SCHEMA]
  [LOGS_DAILY]
  [METRICS_DAILY]
  ```

**Risk if skipped:** Partial answers, hallucinated joins

**Priority:** üî• P0

**Status:** ‚è≥ Not Started

---

### 2Ô∏è‚É£ Hard Grounding Enforcement at Answer Post-Processing

You already added the **prompt grounding guard**, but the final missing piece is:

‚úÖ Add a **post-response validator**:

* If model answers something **not present in retrieved context**
* ‚Üí Replace answer with:

  ```
  "This information is not available in the current metadata."
  ```

**Risk if skipped:** Silent hallucinations in prod

**Priority:** üî• P0

**Status:** ‚è≥ Not Started

---

### 3Ô∏è‚É£ Deterministic Table + Entity Resolution

Right now, if user asks:

> "What's the schema of transactions?"

You still need:

* Fuzzy resolution:
  * `transactions` ‚Üí `transaction_keyspace.dda_transactions`
* Backed by:
  * Alias map
  * Data catalog table

**Risk if skipped:** Wrong table ‚Üí wrong answer

**Priority:** üî• P0

**Status:** ‚è≥ Not Started

---

## üöÄ **P1 ‚Äî HIGH ROI (Scale & Performance)**

These protect you as **document volume grows into millions**.

### 4Ô∏è‚É£ Partitioning Strategy for `rag_documents`

You should implement **range partitioning** on:

```sql
PARTITION BY RANGE (event_date)
```

With:

* Monthly partitions for logs & metrics
* Static partition for metadata

**Why:**

* 10x faster vector scans
* Faster purges
* Lower index bloat

**Priority:** üöÄ P1

**Status:** ‚è≥ Not Started

---

### 5Ô∏è‚É£ Automated Retention & Purge Jobs

You already use:

* `daysBack = 180`

Now you should enforce:

* Auto-delete:

  ```
  logs_daily > 30 days
  logs_weekly > 365 days
  metrics_daily > 90 days
  ```

Using:

* Spring Batch OR Spark job OR Yugabyte cron

**Priority:** üöÄ P1

**Risk if skipped:** Vector index explosion + slow recall

**Status:** ‚è≥ Not Started

---

### 6Ô∏è‚É£ Hot vs Cold Index Strategy

Split into:

* **Hot HNSW Index**
  * logs_daily
  * metrics_daily

* **Cold HNSW Index**
  * schema_metadata
  * business_metadata
  * lineage

**Why:**

* Faster real-time RCA queries
* No contention between live logs and static metadata

**Priority:** üöÄ P1

**Status:** ‚è≥ Not Started

---

## üß† **P2 ‚Äî QUALITY & INTELLIGENCE**

These dramatically improve **answer accuracy and observability**.

### 7Ô∏è‚É£ Adaptive Similarity Threshold Learning

You already have:

* Static thresholds per doc type

Next step:

* Log:

  ```
  question ‚Üí similarity ‚Üí answer accepted/rejected
  ```

* Train:
  * Auto-adjust thresholds per subtype

**Result:** Higher recall without lowering precision

**Priority:** üß† P2

**Status:** ‚è≥ Not Started

---

### 8Ô∏è‚É£ RAG Answer Quality Scoring (LLM-as-a-Judge)

Add a second LLM call:

* Input: Question + Context + Answer
* Output:
  * Grounded? (Yes/No)
  * Missing fields?
  * Confidence score

Store in:

* `rag_answers_audit` table

**Priority:** üß† P2

**This is mandatory for enterprise compliance later.**

**Status:** ‚è≥ Not Started

---

### 9Ô∏è‚É£ Query Rewriting with LLM (Hybrid Mode)

You currently use:

‚úÖ Rule-based canonical templates

Next evolution:

* If confidence < 0.7:
  * Ask Phi-4 to rewrite into canonical form
  * Then re-embed + retry

This creates a **self-healing RAG pipeline**.

**Priority:** üß† P2

**Status:** ‚è≥ Not Started

---

## üèóÔ∏è **P3 ‚Äî PLATFORM SCALE & DEV EXPERIENCE**

These help your **platform grow without friction**.

### üîü Backfill & Re-Embedding Strategy

When you:

* Upgrade embedding model
* Change canonical templates
* Add new metadata

You need:

* Versioned embeddings:

  ```
  embedding_v1
  embedding_v2
  ```

* Online + offline backfill jobs

**Priority:** üèóÔ∏è P3

**Status:** ‚è≥ Not Started

---

### 1Ô∏è‚É£1Ô∏è‚É£ Canary RAG Evaluation Harness

Build:

* A fixed test dataset:
  * 100 questions
  * Expected answers
* Run nightly regression:
  * Recall@1
  * Hallucination rate
  * Latency

**Priority:** üèóÔ∏è P3

**Status:** ‚è≥ Not Started

---

### 1Ô∏è‚É£2Ô∏è‚É£ Multi-Cluster Federated RAG

You already have:

‚úÖ `cluster_name`

Next step:

* Query:
  * Single cluster
  * Or federated across:
    * cassandra-prod
    * cassandra-staging
    * yugabyte-analytics

**Priority:** üèóÔ∏è P3

**Status:** ‚è≥ Not Started

---

## üîÆ **P4 ‚Äî ADVANCED AIOPS (Nice-to-Have, High Differentiation)**

### 1Ô∏è‚É£3Ô∏è‚É£ Predictive Incident Detection

Use:

* metrics_weekly trends
* logs_weekly error slopes

Ask:

> "Will dda_transactions likely fail today?"

This is:

* Predictive RAG + ML hybrid

**Priority:** üîÆ P4

**Status:** ‚è≥ Not Started

---

### 1Ô∏è‚É£4Ô∏è‚É£ Auto-RCA Graph Builder

From:

* logs
* spark lineage
* kafka lineage
* API lineage

Auto-generate:

* Failure propagation graph

**Priority:** üîÆ P4

**This becomes your AppDynamics replacement.**

**Status:** ‚è≥ Not Started

---

### 1Ô∏è‚É£5Ô∏è‚É£ Natural Language ‚Üí Operational Action

Allow:

> "Pause the Spark job writing to dda_transactions"

Pipeline:

UI ‚Üí RAG ‚Üí Tool Router ‚Üí Spark API

**Priority:** üîÆ P4

**Status:** ‚è≥ Not Started

---

## üóÑÔ∏è **INFRASTRUCTURE & SCHEMA EVOLUTION**

### 1Ô∏è‚É£6Ô∏è‚É£ Migrate to TIMESTAMPTZ for Hour-Level RCA

**Current State:**
* Using `event_date DATE` with `LocalDate` in Java
* ‚úÖ Valid and correct for day-level granularity
* ‚úÖ Validated with real data

**Why Migrate:**
* Enable hour-level RCA queries
* Support timezone-aware operations
* Better for distributed systems
* Future-proof for predictive analytics

**Migration Plan:**
* Add `event_timestamp TIMESTAMPTZ` column
* Migrate existing `event_date` ‚Üí `event_timestamp` (set to midnight UTC)
* Update Java models to use `Instant` or `LocalDateTime`
* Update SQL queries to use `event_timestamp`
* Keep `event_date` for backward compatibility (computed column)
* Update date filtering to use `event_timestamp >= :fromTimestamp AND event_timestamp <= :toTimestamp`

**Priority:** üèóÔ∏è P3 (Do before hour-level RCA features)

**Status:** ‚è≥ Not Started

**Dependencies:**
* Complete P0 items first
* Test with existing data

---

## ‚úÖ FINAL EXECUTION ROADMAP (CONDENSED)

### ‚ö° Do Immediately (P0 Performance)

0. **Deploy CPU optimizations to Docker container** (Code ready, needs deployment)

### üî• Do Immediately (P0 Critical)

1. Multi-intent vector fusion
2. Post-answer grounding validator
3. Deterministic table/entity resolution

---

### üöÄ Do After Stabilization (P1)

4. Yugabyte partitioning
5. Auto-retention jobs
6. Hot vs cold vector indexes

---

### üß† Intelligence Layer (P2)

7. Adaptive similarity thresholds
8. LLM-as-a-judge evaluator
9. LLM-assisted query rewriting fallback

---

### üèóÔ∏è Scale & Ops (P3)

10. Embedding backfill versioning
11. Canary regression suite
12. Federated multi-cluster RAG
13. **Migrate to TIMESTAMPTZ** (for hour-level RCA)

---

### üîÆ AIOps Leadership (P4)

14. Predictive failures
15. Auto-RCA graphs
16. Natural-language ops execution

---

## üìä Progress Tracking

| Priority | Total Items | Completed | In Progress | Not Started |
|----------|-------------|-----------|-------------|-------------|
| P0 (Performance) | 1 | 0 (Code) | 1 (Deployment) | 0 |
| P0 (Critical) | 3 | 0 | 0 | 3 |
| P1 | 3 | 0 | 0 | 3 |
| P2 | 3 | 0 | 0 | 3 |
| P3 | 4 | 0 | 0 | 4 |
| P4 | 3 | 0 | 0 | 3 |
| **Total**| **17**      | **0**     | **1**       | **16**      |

**Note:** P0 Performance optimization code is complete but needs deployment to Docker container.

---

## üìù Notes

* All items are prioritized based on impact, risk reduction, and production scalability
* P0 items protect correctness and safety
* P1 items protect scale and performance
* P2 items improve quality and intelligence
* P3 items enable platform growth
* P4 items provide advanced AIOps capabilities

---

**Next Review Date**: TBD

---

## üéØ Recent Updates (2025-12-06)

### ‚úÖ Completed
* CPU optimization code (P0 Performance) - All code changes complete in `api_server.py`
  * Context window: 2048 ‚Üí 512
  * Threading: Explicit (cores-1)
  * Memory optimizations: use_mmap=True, n_threads_batch

### ‚ö†Ô∏è In Progress
* CPU optimization deployment - Code ready, needs to be applied to Docker container

### üìù Documentation Added
* `CPU_OPTIMIZATION_GUIDE.md` - Complete implementation guide
* `OPTIMIZATION_STATUS.md` - Current status and deployment checklist
* `CPU_OPTIMIZATION_REVIEW.md` - Review of additional recommendations
* `APPLY_CPU_OPTIMIZATIONS.md` - Step-by-step deployment instructions

